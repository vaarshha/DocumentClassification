{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoDYScw2KWhd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D, LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9jHTf80KXKv"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def resize_with_pad(image: np.array,\n",
        "                    new_shape: Tuple[int, int],\n",
        "                    padding_color: Tuple[int] = (255, 255, 255)) -> np.array:\n",
        "    original_shape = (image.shape[1], image.shape[0])\n",
        "    ratio = float(max(new_shape))/max(original_shape)\n",
        "    new_size = tuple([int(x*ratio) for x in original_shape])\n",
        "    image = cv2.resize(image, new_size)\n",
        "    delta_w = new_shape[0] - new_size[0]\n",
        "    delta_h = new_shape[1] - new_size[1]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=padding_color)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_t = pd.read_csv(\"train_labels.csv\")\n",
        "train_dir= \"/content/datathon/train/train/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpcACs7ZLYxm"
      },
      "outputs": [],
      "source": [
        "X_tr = []\n",
        "Y_tr = []\n",
        "\n",
        "for name in labels_t['id'].values:\n",
        "    temp = cv2.imread(train_dir + str(name)+\".tif\")\n",
        "    temp = cv2.cvtColor(temp,cv2.COLOR_BGR2YUV)\n",
        "    temp[:,:,0] = cv2.equalizeHist(temp[:,:,0])\n",
        "    temp = cv2.cvtColor(temp, cv2.COLOR_YUV2BGR)\n",
        "    temp = resize_with_pad(temp, (256, 256))\n",
        "    temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)\n",
        "    temp = np.expand_dims(temp, axis=2)\n",
        "    X_tr.append(temp)\n",
        "\n",
        "X_tr = np.array(X_tr)\n",
        "X_tr = X_tr.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuONc8lmWxXC"
      },
      "outputs": [],
      "source": [
        "X_tr/=255\n",
        "X_tr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHeHk7xnW9u7"
      },
      "outputs": [],
      "source": [
        "y_train = labels_t['label']\n",
        "y_train=np.array(y_train)\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTgSmI9VXdbz"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(256,256,1),kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
        "    Conv2D(filters=64, kernel_size=(5, 5), activation='relu', padding = 'same',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
        "    Conv2D(filters=64, kernel_size=(5, 5), activation='relu', padding = 'same',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=1, padding=\"same\"),\n",
        "    BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
        "    Dropout(0.2),\n",
        "    Conv2D(filters=128, kernel_size=(7, 7), activation='relu', padding = 'same',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
        "    Conv2D(filters=128, kernel_size=(7, 7), activation='relu', padding = 'same',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
        "    Flatten(),\n",
        "    Dropout(0.2),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1uFaSvXcKkq"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG1gvqB2pSqZ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTr1prrqejgp"
      },
      "outputs": [],
      "source": [
        "! mkdir model_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX9GvgIQcwzG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint_path = '/content/model_checkpoints/weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq='epoch',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8UO338ff57t"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=2, min_lr=0.00001, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q08DJpHTXtBm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IZSoASuXvYM"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_tr,y_train,\n",
        "              batch_size= 32,\n",
        "              epochs=50,\n",
        "              validation_split=0.2,\n",
        "              callbacks=[early_stop,checkpoint,reduce_lr],\n",
        "              shuffle=True,\n",
        "              verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4y09REshlwj"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "model.save_weights('/content/model_checkpoints/weights.best.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKoV5UeFZ8TA"
      },
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "\n",
        "plt.plot(loss_train, 'g', label='Training loss')\n",
        "plt.plot(loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtOyONZPZ9Sw"
      },
      "outputs": [],
      "source": [
        "train = history.history['accuracy']\n",
        "val = history.history['val_accuracy']\n",
        "plt.plot(train, 'g', label='Training accuracy')\n",
        "plt.plot(val, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
